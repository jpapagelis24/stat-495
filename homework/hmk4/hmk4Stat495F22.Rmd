---
title: "Homework 4 - Stat 495"
author: "Justin Papagelis"
date: "Due Wednesday, Oct. 5th by midnight"
output:
  pdf_document:
    fig_height: 3
    fig_width: 5
  html_document:
    fig_height: 3
    fig_width: 5
  word_document:
    fig_height: 3
    fig_width: 5
---

```{r, include = FALSE}
library(mosaic)
options(digits = 6)
library(rpart)
library(partykit)
library(GGally)
library(broom)
library(ggplot2)
library(lmtest) # for likelihood ratio tests

require(gridExtra) 

library(ISLR)
library(glmnet)
library(rpart.plot)
library(lars)    # you may need to install some of these (one time only)
library(leaps)   # to use variable selection methods if desired

```

# Practicing Academic Integrity

If you worked with others or used resources outside of provided course material (anything besides our textbook(s), course materials in Moodle/Git repo, R help menu) to complete this assignment, please acknowledge them below using a bulleted list. 

<!-- ~~~~~~~~~~~~~~~~ YOU MAY BEGIN EDITING BELOW THIS LINE ~~~~~~~~~~~~~~~~ -->

*I acknowledge the following individuals with whom I worked on this assignment:*

Name(s) and corresponding problem(s)

*

*I used the following sources to help complete this assignment:*

Source(s) and corresponding problem(s)

* 

\newpage

# Homework 4 and 5 Purpose

Homework 4 allows you to practice the two new methods from class recently - GLMs and regression/classification trees. Homework 5 serves as a way to practice our write-up of analyses.

In short, you will be performing some analysis in Homework 4 and then writing it up formally for Homework 5. You will receive some general feedback on Homework 4 as a class, and can use it to refine your models for the write-up in Homework 5. In other words, you may change your models between the two assignments, particularly if you find an issue as you review your work from Homework 4. However, you must submit Homework 4 with potential models for the write-up for both a GLM and tree as discussed below. 

# Homework 4 - Analysis 

For our analysis, we will use the King County, Washington (State) house sales data set, which I am re-hosting from Kaggle. The Kaggle reference is: https://www.kaggle.com/swathiachath/kc-housesales-data/version/1

```{r}
kchouse <- read.csv("https://awagaman.people.amherst.edu/stat495/kc_house_data.csv", header = T)
```

A data dictionary taken from Kaggle is provided for your use (separate file). 

## Motivation to predict when Price is greater than $500,000

A real estate developer is interested in understanding the features that are predictive of homes selling for more than half a million dollars, and has turned to you, a statistical consultant for help. He wants a model that can be applied to make predictions in this setting and wants to understand how the variables in the model are impacting it.

To practice new techniques from class, in your analysis, you are required to use both an appropriate generalized linear model and tree to address the developer's questions of interest, including a model comparison. 


## Instructions for your Homework 4 submission

The outline for Homework 4 is next, but I want to include information here for you about what you'll need in Homework 5 so that you can include extra information for yourself in your Homework 4 submission, as desired. Look at the end of the assignment for this, and be sure to read it! 

Homework 4 requires the following pieces:

* Exploratory Analysis
* GLM - at least 2 models fit with output and assessment
* Tree - at least 2 models fit with output and assessment

It doesn't matter which you tackle first, the GLMs or the trees. 

Be sure you understand how to read output for both GLMs and trees, as this is material covered on the midterm. 

\newpage

# Exploratory Analysis 

For this analysis, we used the King County, Washington (State) house sales data set (`kchouse`). Reference for data: https://www.kaggle.com/swathiachath/kc-housesales-data/version/1. There is no information on the source of the data. 
The data set contains the following variables:
\begin{itemize}
\item id - notation for a specific house - Numeric
\item date - date the house was sold - String
\item price - price the house sold for in dollars - Numeric
\item bedrooms - number of bedrooms in house - Numeric
\item bathrooms- number of bathrooms per bedroom - Numeric
\item sqft\textunderscore living - square footage of the home - Numeric
\item sqft\textunderscore lot - square footage of the lot - Numeric
\item floors - total floors (levels) in house - Numeric
\item waterfront - house has a waterfront view - Y/N
\item view - house has been viewed - Numeric
\item condition - How good the condition is (overall) - Numeric
\item grade - overall grade given to housing unit (based on King County grading system: 1 poor, 13 excellent) - Numeric
\item sqft\textunderscore above - square footage of house apart from basement - Numeric
\item yr\textunderscore built - the year the house was built - Numeric
\item yr\textunderscore renovated - the year when the house was generated
\item zipcode - the zipcode the house is in - Numeric
\item lat - Latitude coordinate of house - Numeric
\item long - Longitude coordinate of house - Numeric
\item sqft\textunderscore living15 - living room area in 2015 (may or may not have been affected the lot size area) - Numeric
\item sqft\textunderscore lot15 - lot size area in 2015 - Numeric
\end{itemize}

Before we fit any models, we need to make some adjustments to how the data is being treated by R. For example, we need to change the `date` to a date format and change `waterfront` and `zipcode` to be treated as categorical variables instead of numeric variables. We also changed the missing values from `yr_renovated` and `sqft_basement` to be "na" instead of 0.

```{r}
kchouse <- kchouse %>%
  mutate(date = lubridate::mdy(date),
         waterfront = as.factor(waterfront),
         zipcode = as.numeric(as.factor(zipcode)))
kchouse <- kchouse %>%
  mutate_at(c('yr_renovated', 'sqft_basement'), ~na_if(.,0))
```

After examining `yr_renovated` and `sqft_basement`, it appears that 95.8% and 60.7% of the values, respectively, are missing. From the scatterplots below, there does not seem to be any clear relationship between `yr_renovated` and `price` as well as `sqft_basement` and `price`. Therefore, we will drop these predictors when creating our models.

```{r}
sum(is.na(kchouse$yr_renovated))/nrow(kchouse)
sum(is.na(kchouse$sqft_basement))/nrow(kchouse)
```


```{r, warning = FALSE}
fig1 <- ggplot(data = kchouse, aes(x = yr_renovated, y = price)) + 
  geom_point() + 
  labs(title = "Scatterplot of Year Renovated and Price", x = "Year Renovated",
       y = "Price")
fig2 <- ggplot(data = kchouse, aes(x = sqft_basement, y = price)) + 
  geom_point() + 
  labs(title = "Scatterplot of Sq Ft Basement and Price", 
       x = "Square Feet Basement", y = "Price")

```

```{r, include = TRUE, fig.width=9, fig.height=4, message=FALSE}
grid.arrange(fig1, fig2, ncol = 2)
```


Additionally, from the Scatterplot of Bedrooms and Price, there appears to be an outlier so we will remove that observation.

```{r}
ggplot(data = kchouse, aes(x = bedrooms, y = price)) + 
  geom_point() + 
  labs(title = "Scatterplot of Bedrooms and Price", 
       x = "Bedrooms", y = "Price")
```

Since there are a couple variables such as `date`, `id` that do not have a relationship with `price` as seen in the scatterplots below, we will remove those predictors as well. 

```{r}
fig3 <- ggplot(data = kchouse, aes(x = date, y = price)) + 
  geom_point() + 
  labs(title = "Scatterplot of Date and Price", 
       x = "Date", y = "Price")

fig4 <- ggplot(data = kchouse, aes(x = id, y = price)) + 
  geom_point() + 
  labs(title = "Scatterplot of ID and Price", 
       x = "ID", y = "Price")
```

```{r, include = TRUE, fig.width=9, fig.height=4, message=FALSE}
grid.arrange(fig3, fig4, ncol = 2)
```


Additionally, it appears that there are some concerns with multicollinearity between `sqft_living` and other variables such as `sqft_living15` and `sqft_above` so we are going to remove `sqft_living` as well. 

```{r}

fig5 <- ggplot(data = kchouse, aes(x = sqft_living, y = sqft_above)) +
  geom_point() + 
  labs(title = "Scatterplot of SqFt Living and SqFt Above", 
       x = "Sq Ft Living", y = "SqFt Living in 2015")

fig6 <- ggplot(data = kchouse, aes(x = sqft_living, y = sqft_living15)) + 
  geom_point() + 
  labs(title = "Scatterplot of SqFt Living and SqFt Living in 2015", 
       x = "Sq Ft Living", y = "SqFt Living in 2015")
```
```{r, include = TRUE, fig.width=9, fig.height=8, message=FALSE}
grid.arrange(fig5, fig6, ncol = 2)
```

```{r}
kchouse1 <- kchouse %>%
  filter(id != 2402100895) %>%
  select(-c(id,zipcode, yr_renovated, date, sqft_basement, sqft_living))
```

From the scatterplot below of the latitude and the longitude, it seems like the houses are all bunched together in near each other (near Seattle) so the results of this analysis can only be generalized to that region of the state.

```{r}
ggplot(data = kchouse, aes(x = lat, y = long)) +
  geom_point()
```

It also appears that many of the predictors are right-skewed which means that a log transformation could be helpful for a model. For example, as seen below, `sqft_living15` and `sqft_lot15` before and after a  log transformation. This is the same for additional variables as well.

```{r}
fig7 <- gf_dhistogram(~ sqft_living15, data = kchouse) %>%
  gf_labs(title = "Histogram of SqFt Living in 2015")

fig8 <- gf_dhistogram(~ sqft_lot, data = kchouse) %>%
  gf_labs(title = "Histogram of SqFt Lot in 2015")

fig9 <- gf_dhistogram(~ log(sqft_living15), data = kchouse) %>%
  gf_labs(title = "Histogram of SqFt Living in 2015")

fig10 <- gf_dhistogram(~ log(sqft_lot), data = kchouse) %>%
  gf_labs(title = "Histogram of SqFt Lot in 2015")

```

```{r, include = TRUE, fig.width=9, fig.height=4, message=FALSE}
grid.arrange(fig7, fig8, fig9, fig10, ncol = 2)
```


From the histogram below, the distribution of `price` is right-skewed with a median of \$450,000 and an IQR is \$323,000. There appears to be many outliers on the high end for `price`. The minimum of `price` is \$78,000 and the maximum is \$7,700,000. Since the distribution is right-skewed, if we were predicting price, a log-transformation could be useful.

```{r}
gf_dhistogram(~ price, data = kchouse) %>%
  gf_labs(title = "Histogram of Prices")

favstats(~ price, data = kchouse)
```

Next, since we want to know if the price of the house is greater or less than \$500,000, we will transform the price variable so that it is an binary predictor that is `1` if the price of the house is greater than \$500,000 and `0` if the price of the house is less than \$500,000.

```{r}
kchouse1 <- mutate(kchouse1, price = ifelse(price > 500000, 1, 0))
```

The last thing that we will do before creating the models is create a training and test set. The training set is used to create a model and then the test set is used to see how the model performs in making predictions. We will randomly create a 75/25 split for the training/test set. 

```{r}
# create the training and test set for price
set.seed(1)
n <- nrow(kchouse1)
train_index <- sample(1:n, 3/4 * n)
test_index <- setdiff(1:n, train_index)

train <- kchouse1[train_index, ]
test <- kchouse1[test_index, ]
```

\newpage

# GLM Model Fitting

The first models that we are going to be fitting are GLMs (or Generalized Linear Models) which are extensions of ordinary linear regression (fitting a least-squares curve) to cases where the response variables are in an exponential family form. This can be response variables that are binomial, Poisson, gamma, or beta. Since we transformed `price` into a binary variable, it can now be modeled as a binomial distribution. 

First, we are going to fit a logistic model using all of the variables that we have not removed from the data set. We use the training set to create the model. 

```{r}
logmod <- glm(price ~ ., data = train, family = "binomial")
msummary(logmod)
```

It appears from the model that we have 9 significant predictors and we can perform a likelihood ratio test to determine if the overall model is significant.

```{r}
lrtest(logmod)
```

Since we have a significant p-value, it appears that the overall model is significant. Now in order to make predictions using this model, we need to bin the fitted values which we will do below.

```{r}
augprice1 <- augment(logmod, type.predict = "response")
augprice1 <- mutate(augprice1, binprediction = round(.fitted, 0))
tally(~ binprediction, data = augprice1)
9826/(9826+6371)

```

After using 0.5 as a default cutoff, the model classifies 60.7% of the prices as below \$500,000. This is close to the actual percentage of the training data that is classified as below \$500,000 which is 58.1% of the observations. We can try to match this a little better by adjusting the cutoff to which we round the numbers. Finding, the appropriate quantile, we now see that the percentage of the binned fitted values for prices below \$500,000 is 58.4% which is much closer than before.

```{r}
# find out the actual percentage in training set
tally(~ price, data = augprice1)
9415/(9415+6782)

# find the appropriate quantile
with(augprice1, quantile(.fitted, 0.58128))

# re-bin the values and find out the percentage
augprice1 <- mutate(augprice1, binprediction2 = as.numeric(.fitted > 0.454465))
tally(~ binprediction2, data = augprice1)
9455/(9455+6742)
```

Next, we can create a confusion matrix and determine how many of the predictions were correct. From the matrix and calculation below, it appears that 84.0% of the predictions for the training set were correct.

```{r}
with(augprice1, table(price, binprediction2))
(8138+5465)/(8138+5465+1277+1317)
```

We will do the same on the test set to see how the model performs. We follow the same process as above to calculate the predicted values and then bin them using our previous cutoff. The model classifies 58.9% of the prices below \$500,000. 

```{r}
augprice2 <- mutate(test, fitted = predict(logmod, newdata = test, type = "response"))
augprice2 <- mutate(augprice2, binprediction = as.numeric(fitted > 0.454465))
tally(~ binprediction, data = augprice2)
(3181)/(3181+2218)
```

Next, we generate a confusion matrix and it appears that 83.8% of the predictions were correct.

```{r}
with(augprice2, table(price, binprediction))
(2719+1806)/(2719+1806+462+412)
```

```{r, warning=FALSE}

# dDELETE I THINK
# get MSE of training set
augprice1 <- augment(logmod, type.predict = "response")
mean((augprice1$.fitted - augprice1$price)^2)

#get MSE of test set
test4 <- mutate(test, fitted = predict(logmod, newdata = test, type = "response"))
test4 <-filter(test4, fitted != "NA")
mean((test4$fitted - test4$price)^2)

```

Next, we will try creating a smaller model by removing some of the predictors that were not significant in our previous model. We are going to remove `bedrooms`, `long`, and `sqft_lot15`. After creating this model, we then repeated the process and removed other variables that were significant, but did not seem to have much of an impact on the predictions because the other variables were much more influential. The other variables we removed were `sqft_lot`, `floors`, `waterfront`, `view`, `condition`, and  `sqft_above`. 

```{r}
logmod2 <- glm(price ~ bathrooms + grade + yr_built + lat + sqft_living15, 
  data = train, family = "binomial")
msummary(logmod2)
lrtest(logmod2)
```

All of the variables in our reduced model are significant and the model overall is significant as well. Now, we can create some predictions following the same process as above.

```{r}
augprice3 <- augment(logmod2, type.predict = "response")
augprice3 <- mutate(augprice3, binprediction = round(.fitted, 0))
tally(~ binprediction, data = augprice3)
9829/(9829+6386)

```

After using 0.5 as a default cutoff, the model classifies 60.6% of the prices as below \$500,000. After adjusting the quantile, we now see that the percentage of the binned fitted values for prices below \$500,000 is 58.1% which is much closer than before.

```{r}
# find the appropriate quantile
with(augprice3, quantile(.fitted, 0.58128))

# re-bin the values and find out the percentage
augprice3 <- mutate(augprice3, binprediction2 = as.numeric(.fitted > 0.451563))
tally(~ binprediction2, data = augprice3)
9414/(9414+6783)
```

Next, we can create a confusion matrix and determine how many of the predictions were correct. From the matrix and calculation below, it appears that 84.0% of the predictions for the training set were correct.

```{r}
with(augprice3, table(price, binprediction2))
(8119+5486)/(8119+5486+1296+1296)
```

We will do the same on the test set to see how the model performs. We follow the same process as above to calculate the predicted values and then bin them using our previous cutoff. The model classifies 58.6% of the prices below \$500,000. 

```{r}
augprice4 <- mutate(test, fitted = predict(logmod2, newdata = test, type = "response"))
augprice4 <- mutate(augprice4, binprediction3 = as.numeric(fitted > 0.451563))
tally(~ binprediction3, data = augprice4)
(3164)/(3164+2235)
```

Next, we generate a confusion matrix and it appears that 83.1% of the predictions were correct.

```{r}
with(augprice4, table(price, binprediction3))
(2690+1794)/(2690+1794+474+441)
```

Overall, from the two logistic regression models that were created, they both performed very similar to each other with the second one having a slightly lower correct prediction rate. However, for the second model, we removed many predictors that were not influential to the model and therefore were not helping make the predictions. After those variables were removed, the logistic model became much more simple with less predictors. The real estate developer's questions of interest is to find the variables that are indicative of houses that sell for over \$500,000 and by keeping a similar performance of the model, but removing extraneous predictors, we are able to find the predictors that are the most important. The second model contained the predictors: `bathrooms`, `grade`, `yr_built`, `lat`, and `sqft_living15`. 


\newpage

# Tree Fitting

```{r}
# choose and fit at least 2 different models for your tree
# include appropriate output and assessment of their performance

# think ahead: what does a reader need to know to follow your model fitting process?
# you may want to jot down notes for yourself that you'll need later!
```

The second method that we are going to be using to fit models is through classification trees. Classification trees are a nonparametric method of prediction and are easy to interpret. Classification trees are used when the response variable is binary so it is appropriate to use them in order to predict if the price is over or under \$500,000. They are created through recursive partitioning to create "splits" between observations with similar qualities.

We will first create a tree with the default stopping criteria.

```{r}
price.rpart1 <- rpart(price ~ ., data = train, method = "class")
printcp(price.rpart1)
rpart.plot(price.rpart1)
```

The classification tree above uses the predictors, `grade`, `lat`, `sqft_above`, `sqft_living15`, `sqft_lot15`, and `yr_built`. The error from this classification tree is 0.8589 which is calculated from the root node error and the relative error in the output above.

```{r}
1 - 0.3368*0.4187
```

Next, we will using the classification tree that we created to see how well it predicts the training set. From the confusion matrix below, it appears that the classification tree predicts correctly for 85.9% of the observations.

```{r}
train1 <- mutate(train, predprice = predict(price.rpart1, type = "class"))
tally(predprice ~ price, data = train1)
(8227+5696)/(8227+5696+1188+1096)

```

And doing the same thing for the test we, it appears that the model predicts correctly 85.5% of the observations.

```{r}
test1 <- mutate(test, predprice = predict(price.rpart1, type = "class", newdata = test))
tally(predprice ~ price, data = test1)
(2748+1870)/(2748+1870+398+383)
```

Before we create the second model, we examine the cross -validation process that was used in creating the previous model. The default cross-validation is a 10-fold cross-validation. Below we can see that as the complexity parameter reaches 0.16, the relative error calculated from the cross-validation decreases only a small amount as the complexity parameter increases. Therefore, we are going to use a complexity parameter of 0.016 for our next tree. 


```{r}
plotcp(price.rpart1)
```

We are also going to control `minsplit = 500` which means that there needs to be 500 observations in a node before a split is attempted. This is reasonable for a tree created from our training data with a size of 16197. We also set `minbucket = 300` which means that the minimum number of observations in any terminal node is 300 observations. There controls are to try and attempt to build a smaller tree while keeping the same effectiveness of the first model.

```{r}
price.control <- rpart.control(minsplit = 500, minbucket = 300, cp = 0.016)
price.rpart2 <- rpart(price ~ ., data = train, method = "class", control = price.control)
printcp(price.rpart2)
```

The classification tree above uses the predictors, `grade`, `lat`, and `sqft_living15`. The error from this classification tree is 0.8303 which is calculated from the root node error and the relative error in the output above.

```{r}
1 - 0.4053*0.4187
```

From the output below, the classification tree we created is much more simple.

```{r}
rpart.plot(price.rpart2)
```

We will now see how the model performs in classifying observations.

```{r}

train2 <- mutate(train, predprice = predict(price.rpart2, type = "class"))
tally(predprice ~ price, data = train2)
(8054+5394)/(8054+5394+1361+1388)
```

It appears the the model classifies observations 83.0% correctly in the training set. We will do the same for the test set.

```{r}

test2 <- mutate(test, predprice = predict(price.rpart2, type = "class", newdata = test))
tally(predprice ~ price, data = test2)
(2710+1772)/(2710+1772+421+496)
```

It appears the the model classifies observations 83.0% correctly in the test set as well. 

From these two models, the first one is more complex and predicts correctly about 2.5-3% better than the second model. However, this means that the first classification tree is less interpretable than the second one. The first tree has 11 splits whereas the second one only has 5. Because of this reason, we would choose the second model over the first one. The slight decrease in correct predictions matters less than the increase in interpretability for the model. Additionally, the error of the second classification tree is slightly less than that of the first tree. Also, since we are determining which of the variables are the best predictors if a house is above \$500,000, the second model only uses 3 predictors whereas the first model uses 6. The predictors used in the second model are `grade`, `lat`, and `sqft_living15`.


\newpage

# Thinking about Homework 5

The eventual Homework 5 submission will include the following sections (along with all code necessary to reproduce your results):

* Introduction and Exploratory Data Analysis - could be two separate sections
* Your GLM and relevant details
* Your Tree and relevant details (can be before or after your GLM)
* Your Model Comparison (can be woven in sections above)
* Your Conclusion

Descriptions of the purpose for each section follow. The idea here is to explain why you'd write each section, and you can work out what needs to be in each in order to fulfill that purpose. 

* Introduction - The real estate developer has interests, but will you be able to address them? How do you understand the tasks you are presented with? It's important to state what you will be doing in the analysis, so the reader can make sure their understanding lines up with what you will be doing. The reader also needs an introduction to your data, either in this section, or below. They need enough detail to be able to determine if your actions later in the analysis are reasonable. 

* Exploratory Data Analysis - You are the analyst here. That means you can use variable re-expressions, subsets of observations, and employ other analytic practices (e.g. training/testing data sets) at your discretion to aid in your analysis, so long as you explain your rationale for them. The reader needs to know what you found and what you decided to do about it, because this has impacts on the rest of your analysis. Remember our lessons from the first classes - be sure you look at the data! Here's your chance to share what you found based on how it impacts your analysis. 

* GLM and Tree sections - These are new techniques. By having each in their own section, you can focus on your explanation for them one at a time. The reader doesn't know when to use these methods or what they do. You have options that you need to pick for the different techniques (various tuning parameters) - for example, are you using a classification or a regression tree? What tree stopping options are being used? What input variables are you using and why? What type of GLM are you using? How will you be measuring performance? Be sure you discuss what options you are selecting and what made you choose those values. Helping explain this shows your understanding of the techniques (remember the class example that minbucket of 30 was nonsensical for the iris data). 

You also want these sections to show off your ability to build models. It is not appropriate to just fit kitchen sink models (as your only model) or accept all default settings without exploring to see if that is really what is best for the task at hand. Your write-up should make it clear what you explored, but you don't have to show every model you considered. 

Finally, remember to check out your "best" model for each technique. Did you check reasonable diagnostics? Does the model make sense? Are variables behaving as you expect? If you focus solely on model performance, that's missing the point. For example, you might find a model has very little error because a variant of the response was accidentally included as a predictor. You are responsible for checking over the model before reporting it. Your write-up should convey that you've done this (the reader can't read your mind to know you did it).

* Model Comparison - There are several ways to compare the models - performance, interpretability, variables involved, etc. Remember you are trying to address the real estate developer's questions of interest, so you probably need to focus on just one model, or maybe you found a way to combine results from a "best" GLM and a "best" tree. The idea for this section is that you need to convey the process used to pick a final model(s) to use to answer those questions, with justification for your choices.  

* Conclusion - This section should be a stand alone summary of your analysis. It is where you get to state your final model and a quick summary of the process used to get there. You also need to address the developer's questions, and this is your last place to do that! Remember to flush out the details here. If your final sentence is "Model 5 is the model I choose and it answers your questions", does it? Are you doing your job as the analyst to leave things at that? For that matter, what is Model 5? 

* Printing Trees - It is fine to print your trees out to separate .pdfs (just be sure to include them in your submission!). Remember if you want to include the .pdf as an image, you have example code for that. Please do show your code though (so no echo = FALSE), and remember that your work should be reproducible. 

* Audience - For purposes of this submission, remember that the audience is the real estate developer, so you'll probably need to include some explanations that you'd leave out of a normal homework. For example, the developer isn't going to know what a GLM is, or what type you are using, or why you'd use it. You should think about where that information should go, and do your best to explain what you need in order to report your findings. Similarly, assume that you found this data to assist the developer and they need basic details about it to understand the variables. They didn't hand it over to you. 


